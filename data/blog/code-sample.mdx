---
title: Is AI training really scalable?
date: '2025-01-25'
tags: ['AI', 'Computer Science', 'OP-ED']
draft: false
summary: Are we being lied to about the future of AI?
---

Modern LLMs like ChatGPT stem from the transformer architecture based on the Google paper titled Attention is all you need **^1**. While the first few models like GPT versions 3 and 3.5 were extremely impressive, later versions such as 4 and 4o have failed to deliver incrementally better results, despite exponentially increasing costs in training. This begs the question— are we approaching a dead end in training such models?

This is a million-dollar question, as it has consequences for the US stock market, many of our careers, and potentially a worst-case apocalyptic AGI scenario **^2**. Currently, Wall Street and Silicon Valley strongly believe that better AI stems out of scale— continuing to pump billions into distributed training with more compute, more data, and larger model sizes, and we’ll achieve greater intelligence and fewer hallucinations. OpenAI has raised 21.9 billion USD by preaching this philosophy **^3**. As we’ll explore soon, there’s a three-fold set of problems that big AI has to tackle - a scaling one, an architectural one, and a data one.

The most significant problem preventing us from achieving “higher intelligence” is caused by the law of diminishing returns for transformer-based language models. Current multimodal models experience a log-linear scaling trend during pre-training, meaning that making linear improvements to the model takes exponentially more compute. Let’s say it costs you 10 million USD to make a 10% zero-shot improvement to your existing model. That means it would cost you a whopping 100 million USD to make a further 10% improvement. **^4^5**

Another problem is the reason why self-driving cars can’t be proven to drive safely. This can be explained by this quote I heard, which I quite liked - AI doesn't "sometimes" hallucinate. AI is always hallucinating and most of the time its hallucination matches expectations. In technical terms, current models perform poorly in long-tailed data distributions, where few data points occur very frequently, while the majority occur rarely. This is a fundamental issue with the current deep learning architecture, which cannot understand causality but simply works based on correlation. If the conditions are freezing and snowy, it won’t be able to infer the relation to slipping due to black ice. Bringing back the self-driving example, these cars seem to drive fine in most normal conditions on which it's primarily trained on, but when presented with an edge-case situation such as a deer crossing, it's more likely to hallucinate— an outcome that is not desirable when you’re a passenger on the road. **^6**

Finally, a model is only as good as the data it is trained on. And data is finite, both in terms of quality and quantity. And as we currently have it, our “brute-force” approach of training intelligent systems is already nearing its limit. Furthermore, the law of diminishing returns presents itself here too, where exponentially more data is required to observe linear improvements in models **^4^5**. To address this issue, synthetic data, aka model-generated data used is used to train other models. Unsurprisingly, it is seen to result in a decline in performance and also undesirable behaviors and biases that are not fully understood. This is often referred to as “Model Autophagy Disorder” or “Habsburg AI” **^7**. This is especially a concern today as more AI generated data is infiltrating the internet, making it hard to differentiate between human and AI synthesized.

## OP-ED:

To me, our current approach to training models is akin to teaching a monkey with learning impairments to multiply two numbers by giving it thousands of multiplication tables and hoping it has seen enough examples not to fail.

So in a broad sense, we can be confident we won’t lose our jobs to AI until we hit a few more milestones—namely, when AI can understand and reason about the world like humans do without needing vast amounts of data; grasping nuances, and handling complex, ambiguous tasks with the same level of intuition and creativity. Until then, AI remains a tool for augmenting human capabilities rather than replacing us entirely.

### References:

1. [Attention Is All You Need(Paper)](https://arxiv.org/abs/1706.03762)
2. Not a reference, but you should watch the show Person of Interest
3. [OpenAI financials page on Crunchbase](https://www.crunchbase.com/organization/openai/company_financials)
4. [Scaling Laws for Neural Language Models(Paper)](https://arxiv.org/pdf/2001.08361)
5. [A Neural Scaling Law from the Dimension of the Data Manifold(Paper)](https://arxiv.org/pdf/2004.10802)
6. [No “Zero-Shot” Without Exponential Data: Pretraining Concept Frequency Determines(Paper)](https://arxiv.org/pdf/2404.04125)
7. [Preventing Al Model Collapse: Addressing the Inherent Risk of Synthetic Datasets(Article)](https://appinventiv.com/blog/ai-model-collapse-prevention/)
